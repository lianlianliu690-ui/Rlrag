import networkx as nx
import argparse
import os

class WordRetriever:
    def __init__(self, gexf_path):
        print(f">>> Loading Knowledge Graph from {gexf_path}...")
        if not os.path.exists(gexf_path):
            raise FileNotFoundError(f"Graph file not found: {gexf_path}")
            
        self.graph = nx.read_gexf(gexf_path)
        print(f"    Graph Loaded! Nodes: {self.graph.number_of_nodes()} | Edges: {self.graph.number_of_edges()}")

    def search_word(self, word, top_k=5):
        """
        æ ¸å¿ƒæ£€ç´¢å‡½æ•°
        1. æŸ¥æ‰¾æ¦‚å¿µèŠ‚ç‚¹ (Concept Node)
        2. æŸ¥æ‰¾åŒä¹‰è¯èŠ‚ç‚¹ (Neighbor Concepts via SIMILAR_TO)
        3. æŸ¥æ‰¾æåŠè¯¥è¯çš„å®ä¾‹ (Semantic Instances)
        """
        target_word = word.lower().strip()
        print(f"\nğŸ” Searching for Word: '{target_word}'")
        
        candidates_concepts = set()
        
        # --- æ­¥éª¤ 1: æŸ¥æ‰¾ç›´æ¥å¯¹åº”çš„æ¦‚å¿µèŠ‚ç‚¹ ---
        # éå†èŠ‚ç‚¹æ‰¾ ID åŒ¹é… (å› ä¸º NetworkX è¯»å–å ID ç±»å‹å¯èƒ½æ˜¯å­—ç¬¦ä¸²)
        direct_match_node = None
        for node, data in self.graph.nodes(data=True):
            # æ¦‚å¿µèŠ‚ç‚¹çš„ç±»å‹é€šå¸¸æ˜¯ "Semantic"
            # æˆ‘ä»¬å…è®¸æ¨¡ç³ŠåŒ¹é… (æ¯”å¦‚ ' good ' æˆ– 'good')
            if data.get('type') == 'Semantic' and node.lower() == target_word:
                direct_match_node = node
                break
        
        if direct_match_node:
            print(f"   âœ… Found Concept Node: '{direct_match_node}'")
            candidates_concepts.add(direct_match_node)
            
            # --- æ­¥éª¤ 2: æŸ¥æ‰¾å›¾è°±ä¸­çš„åŒä¹‰è¯ (Graph Synonyms) ---
            # æ£€æŸ¥æ˜¯å¦æœ‰ SIMILAR_TO è¾¹è¿æ¥çš„é‚»å±…
            # æ³¨æ„è¾¹çš„æ–¹å‘ï¼šå¯èƒ½æ˜¯ Node -> SIMILAR_TO -> Synonym
            neighbors = list(self.graph.neighbors(direct_match_node))
            synonyms = []
            for neighbor in neighbors:
                edge_data = self.graph.get_edge_data(direct_match_node, neighbor)
                # NetworkX çš„ get_edge_data è¿”å›å­—å…¸ï¼Œå¤šé‡å›¾å¯èƒ½è¿”å›å¤šå±‚å­—å…¸
                # è¿™é‡Œåšä¸ªç®€åŒ–å…¼å®¹å¤„ç†
                if edge_data:
                    # å¤„ç†å¤šé‡è¾¹çš„æƒ…å†µ (MultiDiGraph)
                    if isinstance(edge_data, dict) and 0 in edge_data: 
                        attrs = edge_data[0]
                    else:
                        attrs = edge_data
                        
                    if attrs.get('relation') == 'SIMILAR_TO':
                        synonyms.append(neighbor)
                        candidates_concepts.add(neighbor)
            
            if synonyms:
                print(f"   ğŸ”— Found Synonyms in Graph: {synonyms}")
        else:
            print(f"   âš ï¸ Concept Node '{target_word}' not found in Graph (LLM didn't extract it as a key concept).")

        # --- æ­¥éª¤ 3: å¬å›åŠ¨ä½œå®ä¾‹ (Retrieving Instances) ---
        found_instances = []
        
        # ç­–ç•¥ A: é€šè¿‡æ¦‚å¿µèŠ‚ç‚¹å¬å› (Concept -> MENTIONS -> Instance)
        # è¾¹æ–¹å‘é€šå¸¸æ˜¯: Instance -> MENTIONS -> Concept
        # æ‰€ä»¥æˆ‘ä»¬éœ€è¦æ‰¾ Concept çš„ å‰é©±èŠ‚ç‚¹ (Predecessors)
        for concept in candidates_concepts:
            predecessors = list(self.graph.predecessors(concept))
            for pred in predecessors:
                if self.graph.nodes[pred].get('type') == 'Semantic_Instance':
                    found_instances.append((pred, f"Linked to concept '{concept}'"))

        # ç­–ç•¥ B: å…¨æ–‡æ‰«æ (Full-text Fallback)
        # å¦‚æœç­–ç•¥ A æ²¡æ‰¾åˆ°ç»“æœï¼Œæˆ–è€…ä¸ºäº†æ›´å…¨çš„å¬å›ï¼Œæˆ‘ä»¬å¯ä»¥æ‰«æ raw_text
        # (å¯¹äºå‡ åä¸‡èŠ‚ç‚¹çš„å›¾ï¼Œè¿™ä¸ªæ“ä½œä¾ç„¶å¾ˆå¿«ï¼Œæ¯«ç§’çº§)
        print("   ğŸ” Scanning raw text of all instances (Fallback)...")
        scan_count = 0
        for node, data in self.graph.nodes(data=True):
            if data.get('type') == 'Semantic_Instance':
                raw_text = data.get('raw_text', '').lower()
                # ç®€å•çš„å…¨è¯åŒ¹é…ï¼Œé˜²æ­¢ "good" åŒ¹é…åˆ° "goodbye"
                # åœ¨ä¸¤è¾¹åŠ ç©ºæ ¼åŒ¹é…: " good "
                if f" {target_word} " in f" {raw_text} ":
                    # å»é‡ï¼šå¦‚æœå·²ç»åœ¨ç­–ç•¥ A é‡Œæ‰¾åˆ°äº†ï¼Œå°±åˆ«åŠ äº†
                    if not any(x[0] == node for x in found_instances):
                        found_instances.append((node, "Text Match"))
                        scan_count += 1
        
        print(f"   ğŸ“Š Total Semantic Instances Found: {len(found_instances)}")

        # --- æ­¥éª¤ 4: æŸ¥æ‰¾å¯¹é½çš„åŠ¨ä½œæ–‡ä»¶ (Mapping to Motion) ---
        results = []
        for sem_node, reason in found_instances:
            # æŸ¥æ‰¾è¿æ¥çš„ Motion Instance
            # è·¯å¾„: Motion -> ALIGNED_TO -> Semantic
            # æˆ– Semantic -> ALIGNED_TO -> Motion (å–å†³äºå»ºå›¾æ–¹å‘ï¼Œæˆ‘ä»¬ç”¨ predecessors/neighbors å…¼å®¹æŸ¥æ‰¾)
            
            # å…ˆè¯• predecessors (å¦‚æœè¾¹æ˜¯ Motion->Semantic)
            connected_motions = [n for n in self.graph.predecessors(sem_node) if "Motion_Inst" in n]
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¯• neighbors (å¦‚æœè¾¹æ˜¯ Semantic->Motion)
            if not connected_motions:
                connected_motions = [n for n in self.graph.neighbors(sem_node) if "Motion_Inst" in n]
            
            for m_node in connected_motions:
                m_data = self.graph.nodes[m_node]
                results.append({
                    "motion_id": m_node,
                    "file_path": m_data.get('file_path', 'Unknown'),
                    "raw_text": self.graph.nodes[sem_node].get('raw_text', ''),
                    "match_reason": reason,
                    "emotion": m_data.get('emotion_tag', 'neutral')
                })

        # --- æ­¥éª¤ 5: å±•ç¤ºç»“æœ ---
        print(f"\nâœ… Retrieval Results (Top {top_k}):")
        if not results:
            print("   (No motions found)")
        
        # ç®€å•çš„æ’åºï¼šä¼˜å…ˆå±•ç¤ºé€šè¿‡ Concept é“¾æ¥æ‰¾åˆ°çš„ (æ›´å‡†)ï¼Œå…¶æ¬¡æ˜¯æ–‡æœ¬æ‰«æçš„
        results.sort(key=lambda x: 0 if "Linked" in x['match_reason'] else 1)
        
        for i, res in enumerate(results[:top_k]):
            print(f"   [{i+1}] File: {res['file_path']}")
            print(f"       Text : \"{res['raw_text']}\"")
            print(f"       Emo  : {res['emotion']}")
            print(f"       Why  : {res['match_reason']}")
            print("-" * 40)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--gexf", default="/Dataset4D/public/mas-liu.lianlian/output/RAGesture/rl_kg/graph_rag/knowledge_graph_final.gexf", help="Path to Graph")
    parser.add_argument("--word", type=str, default="good", help="Word to search")
    args = parser.parse_args()

    try:
        retriever = WordRetriever(args.gexf)
        retriever.search_word(args.word)
    except Exception as e:
        print(f"Error: {e}")