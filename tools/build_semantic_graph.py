import os
import json
import textgrid
import pandas as pd
import networkx as nx
import argparse
from openai import OpenAI
import numpy as np

from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from dotenv import load_dotenv


import torch
import sys
from mmcv import Config
# æ·»åŠ å¿…è¦çš„è·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.insert(0, '/home/mas-liu.lianlian/RLrag')
from mogen.datasets.builder import build_dataloader, build_dataset


# è¡¥ä¸: è§£å†³ numpy float é—®é¢˜
if not hasattr(np, 'float'):
    np.float = float

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ==========================================
# 1. åŠ¨ä½œåˆ‡ç‰‡ç´¢å¼•æ„å»ºå™¨ (åŸºäº Dataloader)
# ==========================================
# ==========================================
# 1. åŠ¨ä½œåˆ‡ç‰‡ç´¢å¼•æ„å»ºå™¨ (åŸºäº Dataloader + ç»å¯¹æ—¶é—´)
# ==========================================
class MotionDataloaderIndex:
    """
    ä½¿ç”¨ Dataloader è¯»å– LMDB ä¸­é¢„å­˜çš„ 'abs_start_time'ã€‚
    è¿™æ˜¯æœ€ç²¾ç¡®çš„å¯¹é½æ–¹å¼ï¼Œå®Œå…¨æ¶ˆé™¤äº†æ­¥é•¿è®¡ç®—è¯¯å·®ã€‚
    """
    def __init__(self, dataset_cfg_path, device='cuda'):
        self.dataset_cfg_path = dataset_cfg_path
        self.device = device
        self.index = defaultdict(list) 
        
        print(f"Loading Config from {dataset_cfg_path}...")
        self.cfg = Config.fromfile(dataset_cfg_path)
        
        # æˆ‘ä»¬ä»ç„¶éœ€è¦ FPS æ¥è®¡ç®—ç»“æŸæ—¶é—´ (Duration = Length / FPS)
        self.fps = self.cfg.get('pose_fps', 15)  # 15
        if hasattr(self.cfg.data, 'train') and hasattr(self.cfg.data.train, 'fps'):
             self.fps = self.cfg.data.train.fps
             
        print(f">>> Dataset Params | FPS: {self.fps}")
        print(">>> Note: Using pre-calculated 'abs_start_time' from LMDB.")

        self.dataloader = self._load_dataloader(dataset_cfg_path)
        self._build_index()

    def _load_dataloader(self, cfg_path):
        """åŠ è½½ Dataloader (ä¿æŒä¸å˜)"""
        cfg = Config.fromfile(cfg_path)
        dataset = build_dataset(cfg.data.train) # ç¡®ä¿è¯»å–çš„æ˜¯åŒ…å«æ–°å­—æ®µçš„æ•°æ®é›†
        dataloader = build_dataloader(
            dataset,
            samples_per_gpu=1,
            workers_per_gpu=4,
            dist=False,
            shuffle=False 
        )
        return dataloader

    def _build_index(self):
        """éå† Dataloaderï¼Œç›´æ¥æå–ç»å¯¹æ—¶é—´"""
        print(">>> Indexing Slices using 'abs_start_time'...")
        
        for batch in tqdm(self.dataloader, desc="Indexing"):
            try:
                # 1. è·å– ID (å¤„ç† batch åˆ—è¡¨å°è£…)
                s_idx = batch['sample_name']    #'2_scott_1_10_10/0'
                if isinstance(s_idx, list): s_idx = s_idx[0]
                if isinstance(s_idx, dict): s_idx = s_idx.get('id', 'unknown')
                
                slice_id = str(s_idx).replace('/', '_') # e.g., ''2_scott_1_10_10_0''
                
                # 2. æ¨å¯¼ Parent ID
                # ä¾ç„¶éœ€è¦è¿™ä¸ªæ¥æŠŠåˆ‡ç‰‡å½’ç±»åˆ°åŒä¸€ä¸ªé•¿å½•éŸ³ä¸‹
                parts = slice_id.split('_')
                if parts[-1].isdigit():
                    parent_id = "_".join(parts[:-1]) # "'2_scott_1_10_10'"
                else:
                    parent_id = slice_id

                # 3. ğŸ”¥ [æ ¸å¿ƒä¿®æ”¹] ç›´æ¥è·å–ç»å¯¹å¼€å§‹æ—¶é—´
                # å¦‚æœæ‚¨é‡æ–°ç”Ÿæˆäº†æ•°æ®ï¼Œbatch ä¸­ä¸€å®šæœ‰ 'abs_start_time'
                if 'abs_start_time' in batch:
                    abs_start = batch['abs_start_time']
                    
                    # Dataloader å¯èƒ½ä¼šæŠŠ float å˜æˆ Tensorï¼Œè¿™é‡Œåšä¸ªè½¬æ¢
                    if isinstance(abs_start, torch.Tensor):
                        abs_start = abs_start.item()
                    elif isinstance(abs_start, list):
                        abs_start = float(abs_start[0])
                    else:
                        abs_start = float(abs_start)
                
                # 4. è®¡ç®—ç»“æŸæ—¶é—´
                # End = Start + (FrameLen / FPS)
                current_len = 150 # é»˜è®¤å€¼
                if 'motion' in batch:
                    # è·å–ç”± dataset è¿”å›çš„çœŸå® motion é•¿åº¦
                    # shape é€šå¸¸æ˜¯ [batch, frames, dim]
                    if hasattr(batch['motion'], 'shape'):
                         # å¯å‘å¼åˆ¤æ–­å“ªä¸ªæ˜¯æ—¶é—´ç»´åº¦ (é€šå¸¸ > 10)
                        shape = batch['motion'].shape
                        current_len = shape[1]
                        
                
                duration = current_len / self.fps
                abs_end = abs_start + duration

                # 5. å­˜å…¥ç´¢å¼•
                self.index[parent_id].append({
                    'slice_id': slice_id,   #2_scott_1_10_10_0
                    'start': float(abs_start), # ç»å¯¹æ—¶é—´
                    'end': float(abs_end)      # ç»å¯¹æ—¶é—´
                })
                
            except Exception as e:
                # print(f"Error indexing batch {s_idx}: {e}")
                continue
        
        print(f"Index built. Found {len(self.index)} parent recordings.")

    def get_slices(self, parent_id):
        return self.index.get(parent_id, [])

# ==========================================
# 2. TextGrid è¯»å–å™¨ (å®Œæ•´ä¸Šä¸‹æ–‡) - ä¿æŒä¸å˜
# ==========================================
class FilteredTextGridReader:
    """
    åŠŸèƒ½ 1: æ ¹æ® CSV ç­›é€‰ç‰¹å®š Split (train/test) å’Œ Speaker çš„æ–‡ä»¶ IDã€‚
    åŠŸèƒ½ 2: è¯»å–è¿™äº›æ–‡ä»¶çš„å®Œæ•´ä¸Šä¸‹æ–‡å’Œå•è¯æ—¶é—´æˆ³ã€‚
    """
    def __init__(self, csv_path, textgrid_dir, target_speakers=[2], split_type='train'):
        self.textgrid_dir = textgrid_dir
        
        # --- 1. æ¢å¤ç­›é€‰é€»è¾‘ ---
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"Split CSV not found: {csv_path}")
        
        print(f"Loading split rules from: {csv_path}")
        try:
            split_rule = pd.read_csv(csv_path)
        except Exception as e:
            raise ValueError(f"Failed to read CSV: {e}")

        # æå– Speaker ID (å‡è®¾æ ¼å¼ "2_scott_...")
        split_rule['speaker_id_int'] = split_rule['id'].astype(str).str.split("_").str[0].astype(int)

        # æ‰§è¡Œç­›é€‰: Type == split_type AND SpeakerID in target_speakers
        self.selected_df = split_rule.loc[
            (split_rule['type'] == split_type) & 
            (split_rule['speaker_id_int'].isin(target_speakers))
        ]
        
        # è·å–åˆæ³•çš„æ–‡ä»¶ ID åˆ—è¡¨
        # æ³¨æ„ï¼šCSV é‡Œçš„ ID é€šå¸¸æ˜¯ "2_scott_0_1_0" (åˆ‡ç‰‡ID) è¿˜æ˜¯ "2_scott_0_1" (é•¿éŸ³é¢‘ID)?
        # å¦‚æœ CSV é‡Œå­˜çš„æ˜¯åˆ‡ç‰‡ IDï¼Œæˆ‘ä»¬éœ€è¦å»é‡å¾—åˆ°çˆ¶ ID
        raw_ids = self.selected_df['id'].tolist()
        
        # é¢„å¤„ç†ï¼šæå–çˆ¶æ–‡ä»¶å (Parent ID)
        # å‡è®¾ csv é‡Œæ˜¯ 2_scott_0_1_0ï¼Œæˆ‘ä»¬è¦æå– 2_scott_0_1
        # å¦‚æœ csv é‡Œæœ¬èº«å°±æ˜¯é•¿éŸ³é¢‘ IDï¼Œè¿™ä¸€æ­¥ä¹Ÿä¸ä¼šå‡ºé”™
        unique_parent_ids = set()
        for rid in raw_ids:
            parts = str(rid).split('_')
            # ç®€å•çš„å¯å‘å¼è§„åˆ™ï¼šé€šå¸¸æœ€åä¸€ä½æ˜¯åˆ‡ç‰‡ç´¢å¼•
            # å¦‚æœæ–‡ä»¶ååƒ 2_scott_0_1ï¼Œåˆ™å®ƒæœ¬èº«å°±æ˜¯çˆ¶ID
            # å¦‚æœæ–‡ä»¶ååƒ 2_scott_0_1_0ï¼Œåˆ™çˆ¶IDæ˜¯ 2_scott_0_1
            # è¿™é‡Œæˆ‘ä»¬ä¸ºäº†ä¿é™©ï¼Œæ£€æŸ¥å¯¹åº” TextGrid æ˜¯å¦å­˜åœ¨
            
            # å°è¯•ç›´æ¥ç”¨ ID
            if os.path.exists(os.path.join(textgrid_dir, f"{rid}.TextGrid")):
                unique_parent_ids.add(rid)
            else:
                # å°è¯•å»æ‰æœ€åä¸€ä½ä½œä¸ºçˆ¶ ID
                parent_candidate = "_".join(parts[:-1])
                if os.path.exists(os.path.join(textgrid_dir, f"{parent_candidate}.TextGrid")):
                    unique_parent_ids.add(parent_candidate)
        
        self.file_ids = list(unique_parent_ids)
        print(f"Filter Result: Found {len(self.file_ids)} unique Parent TextGrids (Split={split_type}).")

    def get_files(self):
        return self.file_ids

    def read_full_text(self, file_id):
        """
        è¯»å–å®Œæ•´ä¸Šä¸‹æ–‡ (ä¿ç•™åŸ FullContextTextGridReader çš„é€»è¾‘)
        """
        path = os.path.join(self.textgrid_dir, f"{file_id}.TextGrid")
        if not os.path.exists(path):
            return None, None
            
        try:
            tg = textgrid.TextGrid.fromFile(path)
            full_text_list = []
            words = [] 
            
            target_tiers = ['words', 'transcript', 'word', 'upper_word']
            
            for tier in tg:
                if tier.name.lower() in target_tiers:
                    for interval in tier:
                        w = interval.mark.strip()
                        if w and w.lower() not in ['<sil>', 'sil', 'sp', '']:
                            full_text_list.append(w)
                            words.append({
                                'word': w,
                                'start': interval.minTime,
                                'end': interval.maxTime
                            })
                    break 
            
            return " ".join(full_text_list), words
        except Exception as e:
            # print(f"Error reading TextGrid {file_id}: {e}")
            return None, None

# ==========================================
# 2. è¯­ä¹‰æå–æ¨¡å— (ä¿æŒä¸å˜)
# ==========================================
class SemanticExtractor:
    def __init__(self, model="qwen-plus"):
        
        self.client = OpenAI(
            # ä½¿ç”¨ DashScope API Key
            api_key="sk-22f3c8747a014e7d81c1678a1d39817e", 
            # å…³é”®ç‚¹ï¼šå°† base_url æŒ‡å‘é˜¿é‡Œäº‘çš„å…¼å®¹æœåŠ¡ç«¯ç‚¹
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.model = model

    def extract_triplets(self, text):
        prompt = f"""
        You are an expert Linguistic Analyst building the Semantic Layer of a Knowledge Graph for a 3D digital human system.
        Your goal is to parse the input speech text into a structured graph that captures **Content**, **Logic**, and **Emotion**.

        Input Text: "{text}"

        ### 1. Schema Definition (Strictly Follow)

        **Node Types**:
        1. "Semantic": Concrete words found in the text (Verbs, Nouns, Adjectives). Normalize lemmas (e.g., "running" -> "run").
        2. "Discourse_Function": Abstract logic nodes. **ONLY use these values**: 
        ["CONTRAST", "CAUSAL", "EMPHASIS", "UNCERTAINTY", "AGREEMENT", "ELABORATION"].
        3. "Emotion": Abstract emotion nodes. **ONLY use these values**: 
        ["happiness", "surprise", "sadness", "neutral", "anger", "contempt", "fear", "disgust"].

        **Edge Relations**:
        1. "BELONGS_TO": Connects a structure word (e.g., "but") to a "Discourse_Function" (e.g., "CONTRAST").
        2. "EXPRESSES": Connects a content word (e.g., "furious") to an "Emotion" (e.g., "anger").
        3. "SIMILAR_TO": Generalization. Connects a specific word to a more common synonym (e.g., "furious" -> "angry").
        4. "IS_A": Hierarchy. Connects a specific concept to a general category (e.g., "apple" -> "fruit").
        5. "CAUSES": [å› æœ/ä¸Šä¸‹æ–‡] Contextual link between two semantic events. (e.g., "accident" -> "shock").

        ### 2. Tasks

        1. **Extract Nodes**: Identify all key content words and logic words. Create "Semantic" nodes for them.
        2. **Map Logic**: If a word indicates a shift in logic (e.g., "however", "so", "actually"), create a "Discourse_Function" node and link them.
        3. **Map Emotion**: If a word carries strong sentiment, create an "Emotion" node and link them.
        4. **Expand Knowledge**: For key content words, add "SIMILAR_TO" or "IS_A" edges to general concepts. This helps the system find gestures even for rare words.

        ### 3. Output Format
        Output **strictly valid JSON** containing "nodes" and "edges".

        #### Example Input:
        "I was absolutely furious, but I stayed silent."

        #### Example Output:
        {{
            "nodes": [
                {{"id": "furious", "type": "Semantic"}},
                {{"id": "absolutely", "type": "Semantic"}},
                {{"id": "but", "type": "Semantic"}},
                {{"id": "silent", "type": "Semantic"}},
                {{"id": "anger", "type": "Emotion"}},
                {{"id": "CONTRAST", "type": "Discourse_Function"}},
                {{"id": "EMPHASIS", "type": "Discourse_Function"}}
            ],
            "edges": [
                {{"source": "furious", "target": "anger", "relation": "EXPRESSES", "weight": 1.0}},
                {{"source": "furious", "target": "angry", "relation": "SIMILAR_TO", "weight": 0.9}},
                {{"source": "but", "target": "CONTRAST", "relation": "BELONGS_TO", "weight": 1.0}},
                {{"source": "absolutely", "target": "EMPHASIS", "relation": "BELONGS_TO", "weight": 0.8}},
                {{"source": "silent", "target": "quiet", "relation": "SIMILAR_TO", "weight": 0.7}}
            ]
        }}
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    # Qwen æœ‰æ—¶éœ€è¦ System Prompt æ¥å¢å¼ºæŒ‡ä»¤éµå¾ª
                    {"role": "system", "content": "You are a helpful assistant specialized in knowledge graph extraction. Output strictly in JSON format."}, 
                    {"role": "user", "content": prompt}
                ],
                # Qwen çš„å…¼å®¹æ¥å£æ”¯æŒ response_format={"type": "json_object"}
                # ä½†å¦‚æœé‡åˆ°æŠ¥é”™ï¼Œå¯ä»¥å°è¯•å»æ‰è¿™è¡Œï¼ŒQwen å¯¹ Prompt çš„éµå¾ªèƒ½åŠ›é€šå¸¸è¶³å¤Ÿ
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            # è§£æè¿”å›å†…å®¹
            content = response.choices[0].message.content
            # æœ‰æ—¶å€™æ¨¡å‹å¯èƒ½è¿”å› ```json ... ``` åŒ…è£¹çš„æ ¼å¼ï¼Œåšä¸€ä¸ªç®€å•çš„æ¸…æ´—æ›´ç¨³å¥
            if content.startswith("```json"):
                content = content.strip("```json").strip("```")
            elif content.startswith("```"):
                content = content.strip("```")
                
            return json.loads(content)
            
        except Exception as e:
            print(f"Qwen API Error: {e}")
            return None 

# ==========================================
# 3. å›¾è°±æ„å»ºæ¨¡å— (ä¿æŒä¸å˜)
# ==========================================
class SemanticGraphBuilder:
    def __init__(self, output_dir):
        self.graph = nx.DiGraph()
        self.output_dir = output_dir
        if not os.path.exists(output_dir): os.makedirs(output_dir)
        self._init_fixed_nodes()

    def _init_fixed_nodes(self):
        emotions = ["happiness", "sadness", "neutral", "anger", "fear", "surprise", "disgust", "contempt"]
        for e in emotions:
            self.graph.add_node(e, type="Emotion", is_fixed=True)

    def update(self, graph_data, parent_id, slices_info, word_timings):
        """
        graph_data: LLM å…¨å±€åˆ†æç»“æœ
        parent_id: "2_scott_0_1"
        slices_info: æ¥è‡ª Dataloader çš„åˆ‡ç‰‡åˆ—è¡¨
        word_timings: æ¥è‡ª TextGrid çš„å•è¯æ—¶é—´è¡¨
        """
        if not graph_data: return

        # 1. æ·»åŠ å…¨å±€æ¦‚å¿µèŠ‚ç‚¹
        global_concepts = set()
        for node in graph_data.get("nodes", []):
            nid = str(node.get("id", "")).lower().strip()
            if nid:
                global_concepts.add(nid)
                if not self.graph.has_node(nid):
                    self.graph.add_node(nid, type=node.get("type", "Semantic"))
        
        for edge in graph_data.get("edges", []):
            s = str(edge.get("source")).lower().strip()
            t = str(edge.get("target")).lower().strip()
            if s and t: self.graph.add_edge(s, t, relation=edge.get("relation"))

        processed_concepts = {}
        for c in global_concepts:
            # å°† "playing_game" å¤„ç†ä¸º "playing game"
            # å°† "pick_up" å¤„ç†ä¸º "pick up"
            clean_c = c.replace("_", " ").lower().strip()
            processed_concepts[clean_c] = c # Value å¿…é¡»æ˜¯åŸå§‹ ID (å¸¦ä¸‹åˆ’çº¿çš„)

        # 2. æ„å»ºå•è¯æ—¶é—´æŸ¥æ‰¾è¡¨ (åŠ é€ŸåŒ¹é…)
        # word -> [(start, end), ...]
        word_map = defaultdict(list)
        for item in word_timings:
            w_clean = str(item['word']).lower().strip()
            word_map[w_clean].append((float(item['start']), float(item['end'])))

        # 3. ä¸ºæ¯ä¸ª Motion Slice åˆ›å»º Semantic Instance
        for sl in slices_info:
            slice_id = sl['slice_id'] # e.g., 2_scott_0_1_0
            s_start = sl['start']
            s_end = sl['end']
            
            inst_id = f"Semantic_Inst_{slice_id}"   #'Semantic_Inst_2_scott_0_51_51_0'
            
            # --- æ­¥éª¤ A: æ”¶é›†è½åœ¨è¯¥åˆ‡ç‰‡å†…çš„æ‰€æœ‰å•è¯ ---
            slice_word_objs = [] 
            
            for word, timings_list in word_map.items():
                for (w_start, w_end) in timings_list:
                    # åˆ¤å®šé‡å : ä¸ç›¸ç¦»å³é‡å 
                    if not (w_end < s_start or w_start > s_end):
                        slice_word_objs.append({
                            'word': word,
                            'start': w_start
                        })
            
            # --- æ­¥éª¤ B: æŒ‰æ—¶é—´æ’åºå¹¶é‡ç»„å¥å­ ---
            # æŒ‰æ—¶é—´æ’åºï¼Œä¿è¯è¯­åºæ­£ç¡® (e.g. "pick" then "up")
            slice_word_objs.sort(key=lambda x: x['start'])
            
            # æå–çº¯æ–‡æœ¬åˆ—è¡¨
            slice_words_list = [obj['word'] for obj in slice_word_objs]
            
            # æ‹¼æˆå®Œæ•´å­—ç¬¦ä¸²ï¼Œå‰ååŠ ç©ºæ ¼æ–¹ä¾¿å…¨è¯åŒ¹é…
            # e.g. " i am playing game "
            slice_full_text = " " + " ".join(slice_words_list).lower() + " "
            
            # --- æ­¥éª¤ C: çŸ­è¯­çº§åŒ¹é… (Phrase Matching) ---
            related_concepts = set()
            
            # éå†æ‰€æœ‰å…¨å±€æ¦‚å¿µï¼Œçœ‹å®ƒä»¬æ˜¯å¦å‡ºç°åœ¨è¿™ä¸€å°æ®µæ–‡æœ¬é‡Œ
            for clean_concept, original_concept_id in processed_concepts.items():
                # æ£€æŸ¥ " playing game " æ˜¯å¦åœ¨ " i am playing game " é‡Œ
                # è¿™é‡Œåšç®€å•çš„å­ä¸²åŒ¹é…ï¼Œå¯¹äºå¤§å¤šæ•°çŸ­è¯­è¶³å¤Ÿäº†
                if clean_concept in slice_full_text:
                    related_concepts.add(original_concept_id)
                
                # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœæ˜¯å•ä¸ªè¯ï¼Œé˜²æ­¢åŒ¹é…åˆ°å•è¯çš„ä¸€éƒ¨åˆ†
                # ä¾‹å¦‚ concept="act"ï¼Œé˜²æ­¢åŒ¹é…åˆ° "actually"
                # å¯ä»¥åŠ ç©ºæ ¼åˆ¤æ–­: " act " in slice_full_text
                elif " " + clean_concept + " " in slice_full_text:
                     related_concepts.add(original_concept_id)
            
            # åˆ›å»ºå®ä¾‹èŠ‚ç‚¹
            self.graph.add_node(
                inst_id,
                type="Semantic_Instance",
                base_id=slice_id,
                parent_id=parent_id,
                start_time=s_start,
                end_time=s_end,
                raw_text=" ".join(slice_words_list)
            )
            
            # è¿æ¥ Instance -> Global Concepts
            for concept in related_concepts:
                self.graph.add_edge(inst_id, concept, relation="MENTIONS")
            
            # è¿æ¥ Instance -> Global Emotions (Context)
            for node in graph_data.get("nodes", []):
                if node.get("type") == "Emotion":
                    e_nid = str(node.get("id")).lower().strip()
                    self.graph.add_edge(inst_id, e_nid, relation="HAS_CONTEXT_EMOTION")

    def save(self):
        nx.write_gexf(self.graph, os.path.join(self.output_dir, "semantic_layer.gexf"))

# ==========================================
# 5. ä¸»æµç¨‹
# ==========================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset_cfg", required=True, help="Path to dataset config")
    
    # æ¢å¤ CSV å‚æ•°
    parser.add_argument("--csv_path", required=True, help="Path to train_test_split.csv")
    parser.add_argument("--split", default="train", help="train/test/val")
    parser.add_argument("--speaker_id", type=int, default=2)
    
    parser.add_argument("--textgrid_dir", required=True, help="Path to TextGrid directory")
    parser.add_argument("--output_dir", default="data/graph_rag/semantic_final")
    parser.add_argument("--workers", type=int, default=5)
    args = parser.parse_args()

    # 1. å»ºç«‹åŠ¨ä½œç´¢å¼• (ä» Dataloader è·å–åˆ‡ç‰‡ä¿¡æ¯)
    print("--- 1. Building Motion Index from Dataloader ---")
    motion_index = MotionDataloaderIndex(args.dataset_cfg)
    
    # 2. è¯»å– TextGrid (å¸¦ç­›é€‰åŠŸèƒ½)
    print(f"--- 2. Reading TextGrids (Split: {args.split}) ---")
    # ä½¿ç”¨åˆå¹¶åçš„ FilteredTextGridReader
    tg_reader = FilteredTextGridReader(
        csv_path=args.csv_path,
        textgrid_dir=args.textgrid_dir,
        target_speakers=[args.speaker_id],
        split_type=args.split
    )
    files = tg_reader.get_files()
    
    print(f"Found {len(files)} TextGrid files for semantic analysis.")

    # 3. å¹¶è¡Œå¤„ç†
    extractor = SemanticExtractor()
    builder = SemanticGraphBuilder(args.output_dir)
    
    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        future_map = {}
        
        for pid in files:
            full_text, word_timings = tg_reader.read_full_text(pid)
            if not full_text: continue
            
            # æäº¤ LLM ä»»åŠ¡
            future = executor.submit(extractor.extract_triplets, full_text)
            future_map[future] = (pid, word_timings)
            
        for future in tqdm(as_completed(future_map), total=len(future_map), desc="Building Graph"):
            parent_id, word_timings = future_map[future]
            graph_data = future.result()
            
            # ä»ç´¢å¼•ä¸­è·å–åˆ‡ç‰‡ä¿¡æ¯
            slices_info = motion_index.get_slices(parent_id)
            
            if not slices_info:
                # å¯èƒ½æ˜¯å› ä¸º dataset split è¿‡æ»¤æ‰äº†æŸäº›æ–‡ä»¶
                continue
                
            builder.update(graph_data, parent_id, slices_info, word_timings)

    builder.save()
    print("Done!")

if __name__ == "__main__":
    import debugpy
    try:
        # 5678 is the default attach port in the VS Code debug configurations. Unless a host and port are specified, host defaults to 127.0.0.1
        debugpy.listen(("localhost", 9502))
        print("Waiting for debugger attach")
        debugpy.wait_for_client()
    except Exception as e:
      pass
    main()