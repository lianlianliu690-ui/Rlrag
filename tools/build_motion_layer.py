import os
import sys
import torch
import numpy as np
# --- è¡¥ä¸å¼€å§‹ ---
if not hasattr(np, 'float'):
    np.float = float
# --- è¡¥ä¸ç»“æŸ ---
import networkx as nx
import yaml
import json
import argparse
from argparse import Namespace
from tqdm import tqdm
from mmcv import Config

# --- è¡¥ä¸å¼€å§‹: ä¿®å¤ models.motionclip å¯¼å…¥é—®é¢˜ ---
import sys
import os

# 1. å¼ºåˆ¶å°† SynTalker æ ¹ç›®å½•åŠ å…¥ Python æœç´¢è·¯å¾„çš„æœ€å‰é¢
SYNTALKER_ROOT = "/Dataset4D/public/mas-liu.lianlian/code/SynTalker"
MODELS_DIR = os.path.join(SYNTALKER_ROOT, "models")

print(f"DEBUG: Adding path {SYNTALKER_ROOT}")
# æ— è®ºå¦‚ä½•éƒ½æ’åˆ°ç¬¬ä¸€ä¸ªï¼Œç¡®ä¿ä¼˜å…ˆçº§æœ€é«˜ï¼Œé˜²æ­¢åŒååŒ…å†²çª
if SYNTALKER_ROOT not in sys.path:
    sys.path.insert(0, SYNTALKER_ROOT)

if MODELS_DIR not in sys.path:
    sys.path.insert(0, MODELS_DIR) # ğŸ”¥ è¿™ä¸€æ­¥è§£å†³äº† 'No module named temos'

from models.temos.motionencoder.actor import ActorAgnosticEncoder

print("DEBUG: âœ… MotionCLIP Imported Successfully!")
HAS_TMR = True
# --- è¡¥ä¸ç»“æŸ ---
import os
import sys
import warnings

# æ·»åŠ å¿…è¦çš„è·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.insert(0, '/home/mas-liu.lianlian/RLrag')
from mogen.models.transformers.gesture_vae import TransformerVAE
from mogen.datasets.builder import build_dataset, build_dataloader
from mogen.models.utils import rotation_conversions as rc

# ================= 1. TMR Encoder Wrapper (æ–°ç‰ˆ) =================
class TMRMotionWrapper:
    """
    é’ˆå¯¹ TMR (Text-Motion-Retrieval) æ¨¡å‹çš„å°è£…
    è‡ªåŠ¨å¤„ç† Config ç¼ºå¤±å’Œåˆ†ç¦»æƒé‡åŠ è½½é—®é¢˜
    """
    def __init__(self, model_dir, device):
        self.device = device
        print(f">>> [Loader] Loading TMR (ActorAgnosticEncoder) from: {model_dir}")

        # 1. å®ä¾‹åŒ–æ¨¡å‹ (å‚æ•°ç…§æŠ„ SynTalker)
        # æ³¨æ„ï¼šSynTalker ç¡¬ç¼–ç äº† nfeats=623, vae=True, num_layers=4
        # å¦‚æœæ‚¨çš„æ¨¡å‹ä¹Ÿæ˜¯è¿™ä¸€å¥—æƒé‡ (motion_epoch=299.ckpt)ï¼Œè¯·ä¿æŒä¸€è‡´
        self.model = ActorAgnosticEncoder(nfeats=623, vae=True, num_layers=4)
        
        self.model.eval()
        self.model.to(device)

        # 2. åŠ è½½æƒé‡
        # è‡ªåŠ¨å¯»æ‰¾ .ckpt æ–‡ä»¶
        ckpt_path = os.path.join(model_dir, "motion_epoch=299.ckpt") 
        # æˆ–è€…éå†å¯»æ‰¾
        if not os.path.exists(ckpt_path):
             import glob
             ckpts = glob.glob(os.path.join(model_dir, "motion_*.ckpt"))
             if ckpts: ckpt_path = ckpts[0]
        
        if os.path.exists(ckpt_path):
            print(f"    Loading weights from: {ckpt_path}")
            state_dict = torch.load(ckpt_path, map_location=device)
            self.model.load_state_dict(state_dict)
        else:
            print(f"âŒ [Error] Ckpt not found in {model_dir}")

        # 4. å‡å€¼æ–¹å·®åŠ è½½ (Mean/Std Logic)
        self.mean = None; self.std = None
        # å°è¯•å¸¸ç”¨å
        for name in ["mean.npy", "beatx_1-30_amass_h3d_mean.npy"]:
            p = os.path.join(model_dir, name)
            if os.path.exists(p):
                self.mean = torch.from_numpy(np.load(p)).to(device).float()
                break
        for name in ["std.npy", "beatx_1-30_amass_h3d_std.npy"]:
            p = os.path.join(model_dir, name)
            if os.path.exists(p):
                self.std = torch.from_numpy(np.load(p)).to(device).float()
                break
        
        if self.mean is None: print("!! [CRITICAL] No mean.npy found. Normalization disabled!")

    def get_motion_embeddings(self, raw_motion, lengths):
        with torch.no_grad():
            motions = raw_motion.to(self.device).float()
            # å½’ä¸€åŒ–
            if self.mean is not None and self.std is not None:
                motions = (motions - self.mean) / (self.std + 1e-8)
            
            dist = self.model(motions, lengths)
            return dist.loc
    
    def __call__(self, motion_data):
        """
        ä½¿TMRMotionWrapperå¯¹è±¡å¯è°ƒç”¨
        æ¥å—è¿åŠ¨æ•°æ®ï¼Œè¿”å›åµŒå…¥ç‰¹å¾
        """
        # å‡è®¾motion_dataæ˜¯[B, T, D]æ ¼å¼
        batch_size, seq_len, feat_dim = motion_data.shape
        lengths = torch.full((batch_size,), seq_len, dtype=torch.long, device=self.device)
        return self.get_motion_embeddings(motion_data, lengths)

# ================= 2. ä¸»æ„å»ºå™¨ =================
class MotionInstanceBuilder:
    def __init__(self, upper_cfg_path, hands_cfg_path, dataset_cfg_path, device='cuda'):
        self.device = device
        self.output_dir = "/Dataset4D/public/mas-liu.lianlian/output/RAGesture/rl_kg/motion_instances"
        self.motion_assets_dir = os.path.join(self.output_dir, "assets")
        
        if not os.path.exists(self.output_dir): os.makedirs(self.output_dir)
        if not os.path.exists(self.motion_assets_dir): os.makedirs(self.motion_assets_dir)
        
        self.upper_cfg_path = upper_cfg_path
        self.hands_cfg_path = hands_cfg_path
        self.dataset_cfg_path = dataset_cfg_path
        self.graph = nx.DiGraph()

    def _load_single_vae(self, config_path):
        print(f"Loading VAE from {config_path}...")
        with open(config_path, "r") as f:
            cfg = yaml.load(f, Loader=yaml.FullLoader)
        args = Namespace(**cfg)
        model = TransformerVAE(args).to(self.device)
        model.eval()
        
        ckpt_path = args.test_ckpt
        if not os.path.isabs(ckpt_path):
            ckpt_path = os.path.join(os.path.dirname(config_path), ckpt_path)
        
        if os.path.exists(ckpt_path):
            state_dict = torch.load(ckpt_path, map_location=self.device)
            new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.get('model_state', state_dict).items()}
            model.load_state_dict(new_state_dict, strict=False)
        return model

    def _load_motion_encoder(self):
            # 1. å®šä¹‰æ‚¨çš„ TMR æ¨¡å‹ç»å¯¹è·¯å¾„
            # è¯·ç¡®è®¤è¿™ä¸ªè·¯å¾„ä¸‹æœ‰ config.yaml (å¯é€‰) å’Œ .ckpt æ–‡ä»¶
            TMR_DIR = "/Dataset4D/public/mas-liu.lianlian/pretrained_models/beatx_1-30_amass_h3d_tmr/"
            
            print(f">>> [Loader] Attempting to load TMR from: {TMR_DIR}")
            
            # 2. ç‰©ç†æ£€æŸ¥
            if not os.path.exists(TMR_DIR):
                print(f"âŒ [Error] Path not found: {TMR_DIR}")
                return None

            # 3. å°è¯•åŠ è½½
            try:
                # ç¡®ä¿ HAS_TMR ä¸º True (åœ¨æ–‡ä»¶å¤´éƒ¨è¢« import é€»è¾‘è®¾ç½®)
                if not globals().get('HAS_TMR', False):
                    print("âŒ [Error] HAS_TMR is False. MotionCLIP import failed previously.")
                    return None

                # å®ä¾‹åŒ– Wrapper
                encoder = TMRMotionWrapper(TMR_DIR, self.device)
                print("âœ… [Loader] TMR Motion Encoder loaded successfully!")
                return encoder
                
            except Exception as e:
                print(f"âŒ [Critical Error] Failed to load TMR: {e}")
                import traceback
                traceback.print_exc()
                return None

    def _load_dataloader(self, config_path):
        print(f"Loading Dataset from {config_path}...")
        cfg = Config.fromfile(config_path)
        # ğŸ”¥ [æ–°å¢] è·å– FPSï¼Œç”¨äºè®¡ç®—æ—¶é—´ç§’æ•°
        # é»˜è®¤ 15 æˆ– 30ï¼Œå…·ä½“çœ‹ config
        self.fps = cfg.get('pose_fps', 15) 
        if hasattr(cfg.data, 'train') and hasattr(cfg.data.train, 'fps'):
             self.fps = cfg.data.train.fps
        print(f">>> Dataset FPS: {self.fps}")
        return build_dataloader(build_dataset(cfg.data.train), samples_per_gpu=1, workers_per_gpu=4, dist=False, shuffle=False)

    def _preprocess_motion(self, motion_data):
        bs, n, j_raw = motion_data.shape
        num_joints = j_raw // 3
        motion_mat = rc.axis_angle_to_matrix(motion_data.reshape(bs, n, num_joints, 3))
        motion_6d = rc.matrix_to_rotation_6d(motion_mat).reshape(bs, n, num_joints * 6)
        return motion_6d

    def build(self):
        self.upper_vae = self._load_single_vae(self.upper_cfg_path)
        self.hands_vae = self._load_single_vae(self.hands_cfg_path)
        self.motion_encoder = self._load_motion_encoder()
        self.dataloader = self._load_dataloader(self.dataset_cfg_path)

        print(">>> Start Building Motion Instance Graph...")
        count = 0
        
        for batch in tqdm(self.dataloader, desc="Processing"):
            if 'motion_upper' not in batch or 'motion_hands' not in batch: continue

            raw_upper = batch['motion_upper'].to(self.device).float()
            raw_hands = batch['motion_hands'].to(self.device).float()
            # motion = batch['motion'].to(self.device).float()
            # current_len = motion.shape[1]
            # if motion.shape[1] > 150: print(f"  - é•¿åº¦å¤§äº150çš„æ ·æœ¬: {current_len}") 

            # å…ƒæ•°æ®è§£æ
            current_len = raw_upper.shape[1]
            file_id = f"Motion_Inst_{count:06d}"    #Motion_Inst_000000
            speaker_id = "unknown"
            
            # æ³¨æ„: DataLoader çš„ sample_idx å¯èƒ½æ˜¯ list æˆ– tensor
            if 'sample_name' in batch:
                s_idx = batch['sample_name']
                
                
                real_name = str(s_idx[0])
                safe_name = real_name.replace('/', '_').replace('\\', '_')  #2_scott_1_10_10_0
                emotion_tag = self._get_emotion_tag(safe_name)
                file_id = f"Motion_Inst_{safe_name}" #'Motion_Inst_2_scott_1_10_10_0'
                parts = safe_name.split('_')
                speaker_id = parts[0]   # 2

            # 1. è®¡ç®— VAE Latent
            in_upper = self._preprocess_motion(raw_upper)
            in_hands = self._preprocess_motion(raw_hands)
            with torch.no_grad():
                z_u, _ = self.upper_vae.encode_to_dist(in_upper)
                z_h, _ = self.hands_vae.encode_to_dist(in_hands)
                min_len = min(z_u.shape[1], z_h.shape[1])
                vae_latent_np = torch.cat([z_u[:, :min_len], z_h[:, :min_len]], dim=-1).cpu().numpy()

            # 2. è®¡ç®— CLIP Embedding
            clip_emb_np = np.zeros(512)
            if self.motion_encoder:
                with torch.no_grad():
                    # å°è¯•è·å– 263ç»´ å…¨èº«ç‰¹å¾
                    full_body = None
                    full_body = batch['motion_h3d']
                    # if full_body.shape[0] != 150:
                    #     continue                    
                    
                    if full_body is not None:
                        full_body = full_body.to(self.device).float()
                        current_len = full_body.shape[1]
                        if full_body.shape[-1] == 623:
                            emb = self.motion_encoder.get_motion_embeddings(full_body, torch.tensor([current_len]))
                            clip_emb_np = emb.cpu().numpy().flatten()
                        else:
                            # ä»…æ‰“å°ä¸€æ¬¡è­¦å‘Šï¼Œé˜²æ­¢åˆ·å±
                            if count == 0: print(f"[Warn] Feature dim {full_body.shape[-1]} != 263. CLIP skipped.")
            # ==========================================
            # ğŸš€ [æ–°å¢] è§£æç»†ç²’åº¦æ–‡æœ¬ (Word-level Text)
            # ==========================================
            # ğŸš€ [å…³é”®ä¿®æ”¹] è§£æå¹¶å­˜å‚¨å•è¯çº§æ—¶é—´æˆ³
            # ==========================================
            # ç›®æ ‡ï¼šæŠŠ [[[7.43, 7.88], 'okay'], ...] å­˜è¿›èŠ‚ç‚¹å±æ€§
            slice_text = ""
            word_timings_json = "[]" # é»˜è®¤ç©ºåˆ—è¡¨
            
            if 'text_segments' in batch:
                try:
                    # 1. æå–åŸå§‹æ•°æ® (å…¼å®¹ batch ç»´åº¦)
                    raw_segments = batch['text_segments']
                    
                    # å¤„ç†å¯èƒ½çš„åˆ—è¡¨åµŒå¥— (Batch wrapper)
                    if len(raw_segments) > 0 and isinstance(raw_segments[0], list) and isinstance(raw_segments[0][0], list):
                         segments = raw_segments[0] # å–å‡ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„ segments
                    else:
                         segments = raw_segments

                    # 2. æ ¼å¼åŒ–æ•°æ®
                    # æˆ‘ä»¬å¸Œæœ›å­˜æˆ: [{"word": "okay", "start": 7.43, "end": 7.88}, ...]
                    formatted_timings = []
                    word_list = []
                    
                    for item in segments:
                        # item ç»“æ„: [[start, end], 'word']
                        if len(item) >= 2:
                            time_range = item[0] # [7.43, 7.88]
                            word = str(item[1])  # 'okay'
                            
                            formatted_timings.append({
                                "word": word,
                                "start": float(time_range[0]),
                                "end": float(time_range[1])
                            })
                            word_list.append(word)
                    
                    # 3. åºåˆ—åŒ– (å›¾è°±å±æ€§åªèƒ½å­˜å­—ç¬¦ä¸²æˆ–æ•°å€¼ï¼Œä¸èƒ½ç›´æ¥å­˜å¯¹è±¡)
                    slice_text = " ".join(word_list)
                    word_timings_json = json.dumps(formatted_timings)
                    
                except Exception as e:
                    print(f"[Warn] Failed to parse text_segments for {file_id}: {e}")
            # ==========================================
            # ğŸš€ [æ–°å¢] æå–ç»å¯¹æ—¶é—´ (Absolute Time)
            # ==========================================
            abs_start = 0.0
            if 'abs_start_time' in batch:
                val = batch['abs_start_time']
                # å¤„ç† Tensor/List å°è£…
                if isinstance(val, torch.Tensor):
                    abs_start = val.item()
                elif isinstance(val, list):
                    abs_start = float(val[0])
                else:
                    abs_start = float(val)
            
            # è®¡ç®—ç»“æŸæ—¶é—´: Start + (Frames / FPS)
            duration_sec = current_len / self.fps
            abs_end = abs_start + duration_sec
            # ==========================================
            # ==========================================
            # 3. ä¿å­˜èŠ‚ç‚¹ (å­˜å…¥ word_timings)
            # ==========================================
            save_name = f"{file_id}.npy"    #Motion_Inst_2_scott_1_10_10_0
            np.save(os.path.join(self.motion_assets_dir, save_name), {
                'vae_latent': vae_latent_np, 'duration': current_len
            })
            self.graph.add_node(
                file_id, 
                type="Motion_Instance",
                clip_embedding=json.dumps(clip_emb_np.tolist()),
                file_path=f"assets/{save_name}",
                speaker_id=speaker_id, 
                duration=int(current_len),
                # ğŸ”¥ [æ–°å¢] å­˜å…¥ç»å¯¹æ—¶é—´æˆ³ï¼è¿™æ˜¯å¯¹é½çš„æ ¸å¿ƒï¼
                start_time=float(abs_start),
                end_time=float(abs_end),
                
                # ğŸ”¥ æ–°å¢ï¼šå­˜å…¥å®Œæ•´æ–‡æœ¬
                transcript=slice_text, 
                
                # ğŸ”¥ğŸ”¥ æ–°å¢ï¼šå­˜å…¥å•è¯æ—¶é—´ç´¢å¼• (è¿™å°±æ˜¯é‚£ä¸ª db_idx_2_gesture_labels çš„æ›¿ä»£å“ï¼)
                word_timings=word_timings_json,
                
                emotion_tag=emotion_tag, 
            )
            

            count += 1

        nx.write_gexf(self.graph, os.path.join(self.output_dir, "motion_instance_layer.gexf"))
        print(f"Done! Processed {count} instances.")
    def _get_emotion_tag(self, sample_name):
        """
        è§£ææ–‡ä»¶åè·å–æƒ…æ„Ÿæ ‡ç­¾ (æ ¹æ®ç”¨æˆ·æŒ‡å®šï¼šæå–ç¬¬4éƒ¨åˆ†)
        Sample Name: 2_scott_0_1_1  -> å– index[3] = 1 -> "neutral"
        Sample Name: 2_scott_0_73_1 -> å– index[3] = 73 -> "anger"
        """
        try:
            parts = sample_name.split('_')
            
            # ğŸ”¥ ä¿®æ”¹ç‚¹ï¼šæå–ç¬¬ 4 éƒ¨åˆ† (Index 3)
            # ç¡®ä¿åˆ‡åˆ†åçš„é•¿åº¦è¶³å¤Ÿï¼Œé¿å…è¶Šç•Œ
            if len(parts) > 3 and parts[3].isdigit():
                rec_id = int(parts[3])
            else:
                # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸å¯¹ (ä¾‹å¦‚åªæœ‰ 2_scott_0)ï¼Œé»˜è®¤ neutral
                return "neutral" 

            # æ˜ å°„é€»è¾‘ (ä¿æŒä¸å˜)
            if 0 <= rec_id <= 64:
                return "neutral"
            elif 65 <= rec_id <= 72:
                return "happiness"
            elif 73 <= rec_id <= 80:
                return "anger"
            elif 81 <= rec_id <= 86:
                return "sadness"
            elif 87 <= rec_id <= 94:
                return "contempt"
            elif 95 <= rec_id <= 102:
                return "surprise"
            elif 103 <= rec_id <= 110:
                return "fear"
            elif 111 <= rec_id <= 118:
                return "disgust"
            else:
                return "neutral"
                
        except Exception as e:
            # print(f"[Warn] Failed to parse emotion from {sample_name}: {e}")
            return "neutral"
if __name__ == "__main__":
    import debugpy
    try:
        # 5678 is the default attach port in the VS Code debug configurations. Unless a host and port are specified, host defaults to 127.0.0.1
        debugpy.listen(("localhost", 9503))
        print("Waiting for debugger attach")
        debugpy.wait_for_client()
    except Exception as e:
      pass
    parser = argparse.ArgumentParser()
    parser.add_argument("--upper_cfg", type=str, required=True)
    parser.add_argument("--hands_cfg", type=str, required=True)
    parser.add_argument("--data_cfg", type=str, required=True)
    args = parser.parse_args()
    
    MotionInstanceBuilder(args.upper_cfg, args.hands_cfg, args.data_cfg).build()