import os
import glob
import json
import textgrid
import openai
from tqdm import tqdm
from openai import OpenAI
from dotenv import load_dotenv

# ==================== é…ç½®åŒºåŸŸ ====================
dotenv_path = "/home/mas-liu.lianlian/RAG-Gesture/.env"
load_dotenv(dotenv_path)

API_KEY = os.getenv("OPENAI_API_KEY")
BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com") 

TEXTGRID_FOLDER = "/Dataset/mas-liu.lianlian/beat_v2.0.0/beat_english_v2.0.0/textgrid"
OUTPUT_FOLDER = "/Dataset/mas-liu.lianlian/beat_v2.0.0/beat_english_v2.0.0/discourse_rels"

MODEL_NAME = "deepseek-chat" 
# =================================================

print(f"ğŸš€ ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

def extract_tokens_from_textgrid(tg_path):
    """
    æå– TextGrid å†…å®¹ï¼Œå¹¶å±•å¹³æˆä¸€ä¸ªå•ä¸€çš„ Token åˆ—è¡¨ï¼Œä¾›åç»­æŸ¥ç´¢å¼•ä½¿ç”¨ã€‚
    """
    try:
        tg = textgrid.TextGrid.fromFile(tg_path)
    except Exception as e:
        print(f"âŒ TextGrid Error: {tg_path}")
        return "", []
    
    word_tier = None
    possible_names = ["words", "word", "transcript", "MAU"]
    for tier in tg:
        if tier.name.lower() in possible_names:
            word_tier = tier
            break
    if word_tier is None and len(tg) > 0: word_tier = tg[0]
        
    full_text_list = []
    tokens_list = [] 
    
    # è¿™é‡Œçš„ tokens_list å¯¹åº”ä»£ç é‡Œçš„ all_tokens
    if word_tier:
        for idx, interval in enumerate(word_tier):
            text = interval.mark.strip()
            # è¿‡æ»¤æ‰é™éŸ³ï¼Œä½†è¦å°å¿ƒï¼Œå¦‚æœç´¢å¼•é”™ä½å¯èƒ½éœ€è¦ä¿ç•™ç©ºtoken
            # è¿™é‡Œæˆ‘ä»¬å‡è®¾è¿‡æ»¤æ‰é™éŸ³åï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¸GPTç†è§£çš„ä¸€è‡´
            if text and text.lower() not in ["<sil>", "<p>", ""]: 
                full_text_list.append(text)
                tokens_list.append({
                    "surface": text,      
                    "startSec": interval.minTime,
                    "endSec": interval.maxTime
                })
            
    return " ".join(full_text_list), tokens_list

def clean_json_string(json_str):
    if "```json" in json_str:
        json_str = json_str.split("```json")[1].split("```")[0]
    elif "```" in json_str:
        json_str = json_str.split("```")[1].split("```")[0]
    return json_str.strip()

def get_relations_from_gpt(text):
    system_prompt = """
    Extract discourse relations (PDTB style).
    Return JSON object: {"relations": [{"connective": "...", "sense": "...", "arg1": "...", "arg2": "..."}]}
    """
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text}
            ],
            response_format={"type": "json_object"}, 
            temperature=0.1
        )
        result = clean_json_string(response.choices[0].message.content)
        data = json.loads(result)
        
        if isinstance(data, list): return data
        if isinstance(data, dict): return data.get("relations", [])
        return []
    except:
        return []

def find_token_indices(target_text, all_tokens, start_search_idx=0):
    """
    æ ¸å¿ƒå‡½æ•°ï¼šæ ¹æ®æ–‡æœ¬å†…å®¹ï¼Œå» all_tokens åˆ—è¡¨é‡Œæ‰¾åˆ°å¯¹åº”çš„ [ç´¢å¼•å·åˆ—è¡¨]
    ä¾‹å¦‚: target="for example", tokens=[..., "for"(idx 5), "example"(idx 6), ...]
    è¿”å›: [5, 6]
    """
    if not target_text: return []
    
    target_words = target_text.split()
    target_len = len(target_words)
    if target_len == 0: return []

    def clean(s): return s.lower().strip(".,?!\"'")

    # æ»‘åŠ¨çª—å£æœç´¢
    for i in range(start_search_idx, len(all_tokens) - target_len + 1):
        match = True
        for j in range(target_len):
            if clean(target_words[j]) not in clean(all_tokens[i+j]["surface"]):
                match = False
                break
        
        if match:
            # æ‰¾åˆ°äº†ï¼è¿”å›å¯¹åº”çš„ç´¢å¼•åˆ—è¡¨ [i, i+1, i+2...]
            return list(range(i, i + target_len))
            
    return []

def main():
    if not os.path.exists(OUTPUT_FOLDER): os.makedirs(OUTPUT_FOLDER)
    files = glob.glob(os.path.join(TEXTGRID_FOLDER, "*.TextGrid"))
    
    # files = files[0:10] # æµ‹è¯•ç”¨

    print(f"Processing {len(files)} files...")
    for tg_path in tqdm(files):
        base_name = os.path.splitext(os.path.basename(tg_path))[0]
        save_path = os.path.join(OUTPUT_FOLDER, f"{base_name}_whisper_relations.json")
        
        # 1. æå– tokens
        full_text, tokens = extract_tokens_from_textgrid(tg_path)
        
        if not full_text:
            empty_struct = {"sentences": [{"tokens": []}], "relations": []}
            with open(save_path, 'w') as f: json.dump(empty_struct, f)
            continue

        # 2. GPT è·å–æ–‡æœ¬å…³ç³»
        gpt_rels = get_relations_from_gpt(full_text)
        
        # 3. æ„é€  PDTB æ ¼å¼çš„ relation
        pdtb_relations = []
        
        for rel in gpt_rels:
            conn_text = rel.get("connective", "")
            arg1_text = rel.get("arg1", "")
            arg2_text = rel.get("arg2", "")
            sense = rel.get("sense", "Contingency.Cause")

            # æŸ¥æ‰¾ç´¢å¼• (TokenList)
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šæ¯æ¬¡éƒ½ä»å¤´æ‰¾ã€‚æ›´ä¸¥è°¨çš„é€»è¾‘å¯èƒ½éœ€è¦è®°å½•ä¸Šæ¬¡æ‰¾åˆ°çš„ä½ç½®ï¼Œä½†å¯¹çŸ­æ–‡æœ¬é€šå¸¸å¤Ÿç”¨ã€‚
            conn_indices = find_token_indices(conn_text, tokens)
            arg1_indices = find_token_indices(arg1_text, tokens)
            arg2_indices = find_token_indices(arg2_text, tokens)

            # å¿…é¡»è¦æœ‰è¿æ¥è¯çš„ç´¢å¼•ï¼Œå¦åˆ™è¿™æ¡å…³ç³»æ²¡æ³•ç”¨
            if conn_indices:
                pdtb_item = {
                    "Connective": {
                        "RawText": conn_text,
                        "TokenList": conn_indices
                    },
                    "Arg1": {
                        "TokenList": arg1_indices
                    },
                    "Arg2": {
                        "TokenList": arg2_indices
                    },
                    "Sense": [sense] # å¿…é¡»æ˜¯åˆ—è¡¨ ["Contingency.Cause"]
                }
                pdtb_relations.append(pdtb_item)

        # 4. ç»„è£…æœ€ç»ˆ JSON
        final_output = {
            "sentences": [
                {
                    "tokens": tokens # åŸå§‹ tokens åˆ—è¡¨
                }
            ],
            "relations": pdtb_relations
        }

        with open(save_path, 'w') as f:
            json.dump(final_output, f, indent=4)

if __name__ == "__main__":
    main()